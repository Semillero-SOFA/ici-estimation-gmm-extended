{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c272db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.svm import SVC, SVR\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "#accuracy, precision, recall, f1-score\n",
    "GLOBAL_RESULTS_DIR = \"D:/Semillero SOFA/gmm_32_definitivo\"\n",
    "#Datasets Models\n",
    "DATASETS_DIR = f\"{GLOBAL_RESULTS_DIR}/new_models\"\n",
    "\n",
    "# Cargar datos\n",
    "def extract_df(dis, power, gauss, cov):\n",
    "    sub_dir = f\"{dis}km{power}dBm/{gauss}_gaussians\"\n",
    "    df = pd.read_csv(f\"{DATASETS_DIR}/{sub_dir}/models32_gmm_{cov}.csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Calcular metricas\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return r2, rmse, mae\n",
    "# Acumular resultados\n",
    "def accumulate_regression_results(results, tipo = \"train\", r2=None, rmse=None, mae=None):\n",
    "    results[\"r2\"][tipo].append(r2)\n",
    "    results[\"rmse\"][tipo].append(rmse)\n",
    "    results[\"mae\"][tipo].append(mae)\n",
    "\n",
    "def extract_X_y_regression(data, include_osnr=True):\n",
    "    \"\"\"\n",
    "    Extracst features and target variable from the dataset.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset containing features and target.\n",
    "        include_osnr (bool): Whether to include the 'osnr' feature in X.\n",
    "    Returns:\n",
    "        X (pd.DataFrame): The feature set.\n",
    "        y (pd.Series): The target variable (spacing).\n",
    "    \"\"\"\n",
    "    # 1. Preparar datos\n",
    "    data = data.copy()\n",
    "    if data[\"osnr\"].dtype == 'object':\n",
    "        data[\"osnr\"] = data[\"osnr\"].str.replace('dB', '').astype(float)\n",
    "    # Preparar datos\n",
    "    if include_osnr:\n",
    "        X = data.drop([\"spacing\"], axis=1)\n",
    "    else:\n",
    "        # Excluir tanto spacing como osnr\n",
    "        X = data.drop([\"spacing\", \"osnr\"], axis=1)\n",
    "    # Si spacing es categórico (ej: \"29GHz\"), convertir a numérico\n",
    "    y = data[\"spacing\"].copy()\n",
    "    if y.dtype == 'object':\n",
    "        # Extraer números de strings como \"29GHz\"\n",
    "        y = y.str.replace('GHz', '').astype(float)\n",
    "    return X, y\n",
    "def scale_features(X, type = \"standard\"):\n",
    "    \"\"\"\n",
    "    Scales the features using the specified scaling method.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): The feature set to be scaled.\n",
    "        type (str): The type of scaling to apply ('standard' or 'minmax').\n",
    "\n",
    "    Returns:\n",
    "        X_scaled (pd.DataFrame): The scaled feature set.\n",
    "    \"\"\"\n",
    "    if type == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    elif type == \"minmax\":\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported scaling type. Use 'standard' or 'minmax'.\")\n",
    "    \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns)\n",
    "def initialize_regression_results():\n",
    "    results = {}\n",
    "    # Modelo\n",
    "    results[\"model_params\"] = {}\n",
    "    results[\"y_test\"] = []\n",
    "    results[\"y_pred_test\"] = []\n",
    "    results[\"mae\"] = {\"train\": [], \"test\": []}\n",
    "    results[\"r2\"] = {\"train\": [], \"test\": []}\n",
    "    results[\"rmse\"] = {\"train\": [], \"test\": []}\n",
    "    return results\n",
    "\n",
    "# TODO: Agregar más modelos si es necesario\n",
    "def choose_model_regression(model_name):\n",
    "    \"\"\"\n",
    "    Choose and return a regression model.\n",
    "    \n",
    "    Note: For MLP models with Optuna optimization, use train_test_mlp_optuna()\n",
    "    from train_test_optuna.py instead of this function.\n",
    "    \"\"\"\n",
    "    if model_name == \"DecisionTree\":\n",
    "        model = DecisionTreeRegressor(random_state=42)\n",
    "    elif model_name == \"SVM\":\n",
    "        model = SVR(kernel='rbf')\n",
    "    elif model_name == \"RandomForest\":\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo {model_name} no soportado.\")\n",
    "    return model\n",
    "\n",
    "def save_regression_results(results, path_file, gaussian, covariance, model, logger):\n",
    "   \n",
    "    dict_results = {}\n",
    "    metrics = ['mae', 'r2', 'rmse']\n",
    "    for key, value in results.items(): # Iterate over the metrics\n",
    "        if key in metrics:\n",
    "            dict_results[f\"{key}_train\"] = np.mean(value['train'])\n",
    "            dict_results[f\"{key}_test\"] = np.mean(value['test'])\n",
    "            dict_results[f\"{key}_std_train\"] = np.std(value['train'])\n",
    "            dict_results[f\"{key}_std_test\"] = np.std(value['test'])\n",
    "    dict_results['gaussian'] = gaussian\n",
    "    dict_results['covariance'] = covariance\n",
    "    dict_results['model_name'] = model\n",
    "\n",
    "    df_results = pd.DataFrame([dict_results])\n",
    "    if os.path.exists(path_file):\n",
    "        current_results = pd.read_csv(path_file)\n",
    "    else:\n",
    "        current_results = pd.DataFrame()\n",
    "        \n",
    "    current_results = pd.concat([current_results, df_results], ignore_index=True)\n",
    "    current_results.to_csv(path_file, index=False)\n",
    "    log_msg = f\"Saved regression {gaussian} gaussians, {covariance} covariance, model {model} results to {path_file}\"\n",
    "    logger.info(log_msg)\n",
    "\n",
    "def save_regression_results_detailed(results, path_file, gaussian, covariance, model, logger):\n",
    "    \"\"\"\n",
    "    Save raw results (without averaging) for each fold)\n",
    "    It also include the best params for each fold.\n",
    "    Save in a JSON file. \n",
    "    It will save in a specific folder according to the gaussians and covariance type.\n",
    "    TODO: Save in a compressed format and backup if it's necessary\n",
    "\n",
    "        Args:\n",
    "            results (dict): The results dictionary containing metrics and model parameters.\n",
    "            path_file (str): The file path to save the detailed results.\n",
    "            gaussian (int): The number of gaussians used in the model.\n",
    "            covariance (str): The type of covariance used in the model.\n",
    "            model (str): The name of the regression model used.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path_file):\n",
    "        dict_results = {\n",
    "            str(gaussian): {\n",
    "                covariance:{\n",
    "                    model: {\n",
    "                    'metrics': {\n",
    "                        'mae': results['mae'],\n",
    "                        'r2': results['r2'],\n",
    "                        'rmse': results['rmse'],\n",
    "                    },\n",
    "                    'model_params': results['model_params'],\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # If the file exists, save back up and load the existing results\n",
    "        backup_path = path_file + \".bak\"\n",
    "        shutil.copy2(path_file, backup_path)\n",
    "\n",
    "        with open(path_file, \"r\") as f:\n",
    "            dict_results = json.load(f)\n",
    "        # If gaussian type does not exist, create it\n",
    "        if str(gaussian) not in dict_results:\n",
    "            dict_results[str(gaussian)] = {}\n",
    "        # If covariance type does not exist, create it\n",
    "        if covariance not in dict_results[str(gaussian)]:\n",
    "            dict_results[str(gaussian)][covariance] = {}\n",
    "        # Update the results for the specific fold\n",
    "        dict_results[str(gaussian)][covariance][model] = {\n",
    "            'metrics': {\n",
    "                'mae': results['mae'],\n",
    "                'r2': results['r2'],\n",
    "                'rmse': results['rmse'],\n",
    "            },\n",
    "            'model_params': results['model_params'],\n",
    "        }\n",
    "\n",
    "    # Save the updated results\n",
    "    with open(path_file, \"w\") as f:\n",
    "        json.dump(dict_results, f, indent=4)\n",
    "    log_msg = f\"Saved detailed regression {gaussian} gaussians, {covariance} covariance, model {model} results to {path_file}\"\n",
    "    logger.info(log_msg)\n",
    "\n",
    "def setup_logger(name: str) -> logging.Logger:\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),  # Log to console\n",
    "            logging.FileHandler(f\"{name}.log\")\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    return logger\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e9144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros para GridSearchCV (se utiliza solo en funcion train_test_regression_model)\n",
    "PARAMS_GRID_REGRESSION = {\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],\n",
    "        'kernel': ['rbf']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def train_test_regression_model(data, model_name, logger, include_osnr=True):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de regresión Decision Tree para predecir spacing\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame con los datos\n",
    "        model_name: Nombre del modelo a utilizar\n",
    "        include_osnr: Si incluir OSNR como feature o no\n",
    "    \n",
    "    Returns:\n",
    "        dict: métricas del modelo y modelo entrenado\n",
    "    \"\"\"\n",
    "    results = initialize_regression_results()\n",
    "    # 1. Preparar datos\n",
    "    X, y = extract_X_y_regression(data, include_osnr=include_osnr)\n",
    "    #X_scaled = scale_features(X, type=\"standard\")\n",
    "    n_splits = 5\n",
    "    \n",
    "\n",
    "    # TODO: Accoding to GPT: StratifiedKFold stratification: \n",
    "    # using LabelEncoder().fit_transform(y) on continuous spacing is not appropriate — \n",
    "    # it uses unique continuous values and will not produce useful strata (or will fail if few repeats).\n",
    "    #  You should bin y (e.g. pd.qcut or pd.cut) before stratifying.\n",
    "    \n",
    "    # Convertir y a bins para estratificación\n",
    "    y_bins = LabelEncoder().fit_transform(y)\n",
    "    # Crear objeto para estratificación\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    #sin estratificar:\n",
    "    # kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    #for train_index, test_index in kf.split(X_scaled, y):\n",
    "\n",
    "    # Extraer parametros para el modelo\n",
    "    params = PARAMS_GRID_REGRESSION[model_name]\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(X, y_bins)):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        scaler = StandardScaler().fit(X_train) # Ajustar scaler solo con datos de entrenamiento\n",
    "        # Escalar features utilizando solo datos de train\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # 3. Definir modelo\n",
    "\n",
    "\n",
    "        model = choose_model_regression(model_name)\n",
    "\n",
    "        model = GridSearchCV(estimator=model,\n",
    "                             param_grid=params, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        #log(best_model.best_params_.values()) # Imprimir mejores parámetros\n",
    "        # 2 crosvalidacion - 1.\n",
    "        ###################### ttttttt\n",
    "        ################ tttt\n",
    "        # 4. Entrenar modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        logger.info(str(model.best_params_))\n",
    "        # 5. Evaluar modelo\n",
    "        # Predicciones\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        # Métricas\n",
    "        train_r2, train_rmse, train_mae = calculate_regression_metrics(y_train, y_pred_train)\n",
    "        test_r2, test_rmse, test_mae = calculate_regression_metrics(y_test, y_pred_test)\n",
    "        # Resultados\n",
    "        # Acumular resultados\n",
    "        accumulate_regression_results(results, \"train\", train_r2, train_rmse, train_mae)\n",
    "        accumulate_regression_results(results, \"test\", test_r2, test_rmse, test_mae)\n",
    "        results[\"y_test\"].extend(y_test)\n",
    "        results[\"y_pred_test\"].extend(y_pred_test)\n",
    "        results[\"model_params\"][index] = model.best_params_  # Save best_params in each fold\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53994f",
   "metadata": {},
   "source": [
    "## Execute Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d507818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #=====================================================\n",
    "# # Cargar dataset\n",
    "# #=====================================================\n",
    "# distancias = [0, 270]\n",
    "# power = [0, 0, 9]\n",
    "# gaussians = [16,24,32]\n",
    "# covs = [\"diag\", \"spherical\"]\n",
    "# models = [\"SVM\", \"DecisionTree\"]\n",
    "\n",
    "# dis = 0\n",
    "# power = 0\n",
    "# gauss = 16\n",
    "# cov = \"diag\"\n",
    "\n",
    "# database = extract_df(dis, power, gauss, cov)\n",
    "\n",
    "# model_name = \"DecisionTree\"  # Cambiar a \"DecisionTree\" o \"RandomForest\" según se desee\n",
    "\n",
    "# results= train_test_regression_model(database, model_name, include_osnr=True)\n",
    "\n",
    "# save_regression_results(results)\n",
    "\n",
    "# results_wo =  train_test_regression_model(database, model_name, include_osnr=False)\n",
    "\n",
    "# save_regression_results(results_wo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95ba55",
   "metadata": {},
   "source": [
    "## Full Experiment Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae31c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================\n",
    "# Crear Logger\n",
    "#=====================================================\n",
    "timestamp = datetime.datetime.now().strftime(\"%m_%d_%H%M\")\n",
    "run_output_dir = os.path.join(GLOBAL_RESULTS_DIR, 'results', f\"run_{timestamp}\")\n",
    "os.makedirs(run_output_dir, exist_ok=True)\n",
    "\n",
    "logger = setup_logger(run_output_dir)\n",
    "\n",
    "#=====================================================\n",
    "# Parametros\n",
    "#=====================================================\n",
    "\n",
    "distancias = [0, 270]\n",
    "powers = [0, 0, 9]\n",
    "dist_powers = [(0,0), (270,0), (270,9)]\n",
    "gaussians = [16,24,32]\n",
    "covs = [\"diag\", \"spherical\"]\n",
    "# TODO: Agregar más modelos si es necesario\n",
    "# - Agregar MLP -> Modelo Multimodal. => informacion en Agregar optuna.\n",
    "# - Agregar ejecucion segundo plano. tmux. No hup\n",
    "# - Verificar que agrega sobre logger. Hacer copias de seguridad \n",
    "# - Agregar hiperparametros a el csv que se guarda\n",
    "models = [\"DecisionTree\"]\n",
    "\n",
    "#=====================================================\n",
    "# Iterar sobre todos los escenarios (ML models only)\n",
    "#=====================================================\n",
    "# Compute total number of ML runs and create progress bar\n",
    "total_runs = len(dist_powers) * len(gaussians) * len(covs) * len(models)\n",
    "ml_pbar = tqdm.tqdm(total=total_runs, desc=\"ML Model Training Progress\")\n",
    "for distancia, power in dist_powers:\n",
    "    for gaussian in gaussians:\n",
    "        for cov in covs:\n",
    "            database = extract_df(distancia, power, gaussian, cov)\n",
    "            for model_name in models:\n",
    "                #database = extract_df(distancia, power, gaussian, cov)\n",
    "                    #model_name = model  # Cambiar a \"DecisionTree\" o \"RandomForest\" según se desee\n",
    "                    output_dir = os.path.join(run_output_dir, f\"{distancia}_{power}\") # It can be saved in other specific folder\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                    #=====================================================\n",
    "                    results= train_test_regression_model(database, model_name, logger, include_osnr=True)\n",
    "                    filename = os.path.join(output_dir, f'reg_results_w.csv')\n",
    "\n",
    "                    #This function will save the average results\n",
    "                    save_regression_results(results, filename, gaussian, cov, model_name, logger)\n",
    "                    # TODO: ¿Guardar parametros del modelo?\n",
    "                    filename = os.path.join(output_dir, f'reg_results_w_detailed.json')\n",
    "                    save_regression_results_detailed(results, filename, gaussian, cov, model_name, logger)\n",
    "\n",
    "                    #=====================================================\n",
    "                    # Guardar resultados sin OSNR\n",
    "                    #=====================================================\n",
    "                    results_wo =  train_test_regression_model(database, model_name, logger, include_osnr=False)\n",
    "                    filename = os.path.join(output_dir, f'reg_results_wo.csv')\n",
    "                    save_regression_results(results_wo, filename, gaussian, cov, model_name, logger)\n",
    "\n",
    "                    filename = os.path.join(output_dir, f'reg_results_wo_detailed.json')\n",
    "                    save_regression_results_detailed(results_wo, filename, gaussian, cov, model_name, logger)\n",
    "\n",
    "                    log_msg = f\"Completed {model_name} model for {gaussian} gaussians, {cov} covariance, distance {distancia} km and power {power} dBm.\"\n",
    "                    logging.info(log_msg)\n",
    "                    # Update progress bar after finishing this ML configuration\n",
    "                    try:\n",
    "                        ml_pbar.update(1)\n",
    "                    except Exception:\n",
    "                        # If progress update fails (rare), continue without stopping the experiment\n",
    "                        pass\n",
    "# Close the progress bar when done\n",
    "try:\n",
    "    ml_pbar.close()\n",
    "except Exception:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
